{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>task</th>\n",
       "      <th>adherence</th>\n",
       "      <th>file</th>\n",
       "      <th>spectrogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f9475bb-f13b-4f68-969b-28f20455b3e7</td>\n",
       "      <td>Loudness</td>\n",
       "      <td>5.0</td>\n",
       "      <td>/Users/rachelwang/Downloads/bids_with_sensitiv...</td>\n",
       "      <td>/Users/rachelwang/Downloads/notes/models/adher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1f9475bb-f13b-4f68-969b-28f20455b3e7</td>\n",
       "      <td>Respiration-and-cough-Breath-2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>/Users/rachelwang/Downloads/bids_with_sensitiv...</td>\n",
       "      <td>/Users/rachelwang/Downloads/notes/models/adher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f9475bb-f13b-4f68-969b-28f20455b3e7</td>\n",
       "      <td>Respiration-and-cough-FiveBreaths-1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>/Users/rachelwang/Downloads/bids_with_sensitiv...</td>\n",
       "      <td>/Users/rachelwang/Downloads/notes/models/adher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f9475bb-f13b-4f68-969b-28f20455b3e7</td>\n",
       "      <td>Respiration-and-cough-ThreeQuickBreaths-2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>/Users/rachelwang/Downloads/bids_with_sensitiv...</td>\n",
       "      <td>/Users/rachelwang/Downloads/notes/models/adher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1f9475bb-f13b-4f68-969b-28f20455b3e7</td>\n",
       "      <td>Maximum-phonation-time-1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>/Users/rachelwang/Downloads/bids_with_sensitiv...</td>\n",
       "      <td>/Users/rachelwang/Downloads/notes/models/adher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    pid  \\\n",
       "0  1f9475bb-f13b-4f68-969b-28f20455b3e7   \n",
       "1  1f9475bb-f13b-4f68-969b-28f20455b3e7   \n",
       "2  1f9475bb-f13b-4f68-969b-28f20455b3e7   \n",
       "3  1f9475bb-f13b-4f68-969b-28f20455b3e7   \n",
       "4  1f9475bb-f13b-4f68-969b-28f20455b3e7   \n",
       "\n",
       "                                        task  adherence  \\\n",
       "0                                   Loudness        5.0   \n",
       "1             Respiration-and-cough-Breath-2        5.0   \n",
       "2        Respiration-and-cough-FiveBreaths-1        5.0   \n",
       "3  Respiration-and-cough-ThreeQuickBreaths-2        4.0   \n",
       "4                   Maximum-phonation-time-1        5.0   \n",
       "\n",
       "                                                file  \\\n",
       "0  /Users/rachelwang/Downloads/bids_with_sensitiv...   \n",
       "1  /Users/rachelwang/Downloads/bids_with_sensitiv...   \n",
       "2  /Users/rachelwang/Downloads/bids_with_sensitiv...   \n",
       "3  /Users/rachelwang/Downloads/bids_with_sensitiv...   \n",
       "4  /Users/rachelwang/Downloads/bids_with_sensitiv...   \n",
       "\n",
       "                                         spectrogram  \n",
       "0  /Users/rachelwang/Downloads/notes/models/adher...  \n",
       "1  /Users/rachelwang/Downloads/notes/models/adher...  \n",
       "2  /Users/rachelwang/Downloads/notes/models/adher...  \n",
       "3  /Users/rachelwang/Downloads/notes/models/adher...  \n",
       "4  /Users/rachelwang/Downloads/notes/models/adher...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the CSV data\n",
    "data_path = '/Users/rachelwang/Downloads/notes/models/csv/adherence_labeled_with_image.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adherence label counts:\n",
      "adherence\n",
      "1.0     20\n",
      "2.0     11\n",
      "3.0     25\n",
      "4.0     90\n",
      "5.0    395\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each adherence label\n",
    "label_counts = data['adherence'].value_counts().sort_index()\n",
    "print(\"Adherence label counts:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the split CSV files\n",
    "train_csv_path = '/Users/rachelwang/Downloads/notes/models/train_data_adherence.csv'\n",
    "val_csv_path = '/Users/rachelwang/Downloads/notes/models/val_data_adherence.csv'\n",
    "test_csv_path = '/Users/rachelwang/Downloads/notes/models/test_data_adherence.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['spectrogram']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.dataframe.iloc[idx]['adherence'] - 1  # 1-based to 0-based label\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations with augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = SpectrogramDataset(train_df, transform=transform)\n",
    "val_dataset = SpectrogramDataset(val_df, transform=transform)\n",
    "test_dataset = SpectrogramDataset(test_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelwang/Downloads/notes/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rachelwang/Downloads/notes/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/rachelwang/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 26.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the ResNet model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 5)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/50\n",
      "Batch 0, Loss: 0.7622745037078857\n",
      "Batch 10, Loss: 0.452995240688324\n",
      "Epoch [1/50], Loss: 0.7920, Accuracy: 74.60%\n",
      "Validation Loss: 0.7453, Validation Accuracy: 76.85%\n",
      "Starting epoch 2/50\n",
      "Batch 0, Loss: 0.9998501539230347\n",
      "Batch 10, Loss: 0.5594660043716431\n",
      "Epoch [2/50], Loss: 0.8097, Accuracy: 73.67%\n",
      "Validation Loss: 0.7464, Validation Accuracy: 76.85%\n",
      "Starting epoch 3/50\n",
      "Batch 0, Loss: 0.7110664248466492\n",
      "Batch 10, Loss: 0.7684535384178162\n",
      "Epoch [3/50], Loss: 0.7782, Accuracy: 74.60%\n",
      "Validation Loss: 0.7424, Validation Accuracy: 76.85%\n",
      "Starting epoch 4/50\n",
      "Batch 0, Loss: 0.7279840707778931\n",
      "Batch 10, Loss: 0.5718775987625122\n",
      "Epoch [4/50], Loss: 0.8006, Accuracy: 74.60%\n",
      "Validation Loss: 0.7608, Validation Accuracy: 76.85%\n",
      "Starting epoch 5/50\n",
      "Batch 0, Loss: 0.7147996425628662\n",
      "Batch 10, Loss: 1.0089263916015625\n",
      "Epoch [5/50], Loss: 0.7964, Accuracy: 74.13%\n",
      "Validation Loss: 0.7540, Validation Accuracy: 76.85%\n",
      "Starting epoch 6/50\n",
      "Batch 0, Loss: 0.8765460252761841\n",
      "Batch 10, Loss: 0.9395875930786133\n",
      "Epoch [6/50], Loss: 0.8030, Accuracy: 74.13%\n",
      "Validation Loss: 0.7788, Validation Accuracy: 75.93%\n",
      "Starting epoch 7/50\n",
      "Batch 0, Loss: 0.8352822661399841\n",
      "Batch 10, Loss: 0.7685736417770386\n",
      "Epoch [7/50], Loss: 0.7839, Accuracy: 73.90%\n",
      "Validation Loss: 0.7581, Validation Accuracy: 76.85%\n",
      "Starting epoch 8/50\n",
      "Batch 0, Loss: 0.7410074472427368\n",
      "Batch 10, Loss: 0.5924572944641113\n",
      "Epoch [8/50], Loss: 0.8103, Accuracy: 74.13%\n",
      "Validation Loss: 0.7619, Validation Accuracy: 76.85%\n",
      "Starting epoch 9/50\n",
      "Batch 0, Loss: 0.6273894906044006\n",
      "Batch 10, Loss: 0.6464992761611938\n",
      "Epoch [9/50], Loss: 0.7942, Accuracy: 74.13%\n",
      "Validation Loss: 0.7457, Validation Accuracy: 76.85%\n",
      "Starting epoch 10/50\n",
      "Batch 0, Loss: 0.7850545644760132\n",
      "Batch 10, Loss: 0.7168022394180298\n",
      "Epoch [10/50], Loss: 0.7999, Accuracy: 73.44%\n",
      "Validation Loss: 0.7456, Validation Accuracy: 76.85%\n",
      "Starting epoch 11/50\n",
      "Batch 0, Loss: 0.7336470484733582\n",
      "Batch 10, Loss: 0.9165483713150024\n",
      "Epoch [11/50], Loss: 0.8490, Accuracy: 73.67%\n",
      "Validation Loss: 0.7636, Validation Accuracy: 76.85%\n",
      "Starting epoch 12/50\n",
      "Batch 0, Loss: 0.5974084138870239\n",
      "Batch 10, Loss: 0.9268620610237122\n",
      "Epoch [12/50], Loss: 0.7777, Accuracy: 73.90%\n",
      "Validation Loss: 0.7431, Validation Accuracy: 76.85%\n",
      "Starting epoch 13/50\n",
      "Batch 0, Loss: 0.7945300340652466\n",
      "Batch 10, Loss: 0.9528188705444336\n",
      "Epoch [13/50], Loss: 0.8014, Accuracy: 73.90%\n",
      "Validation Loss: 0.7690, Validation Accuracy: 76.85%\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Best model saved at: resnet_best_model_adherence.pth\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_model_path = 'resnet_best_model_adherence.pth'\n",
    "best_val_loss = float('inf')\n",
    "early_stop_patience = 10\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()  # Convert labels to LongTensor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).long()  # Convert labels to LongTensor\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(f\"Best model saved at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8331, Test Accuracy: 75.23%\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).long()  # Convert labels to LongTensor\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
